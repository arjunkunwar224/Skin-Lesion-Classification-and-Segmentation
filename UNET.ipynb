{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to replace tensorflow_addons rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    \"\"\"\n",
    "    Rotates an image (tensor) by a given angle (in radians) using bilinear interpolation.\n",
    "    \"\"\"\n",
    "    # Expand dims to add a batch dimension.\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    cos_val = tf.math.cos(angle)\n",
    "    sin_val = tf.math.sin(angle)\n",
    "    h = tf.cast(tf.shape(image)[1], tf.float32)\n",
    "    w = tf.cast(tf.shape(image)[2], tf.float32)\n",
    "    cx = w / 2.0\n",
    "    cy = h / 2.0\n",
    "    # Compute translation so that rotation is about the center:\n",
    "    tx = cx - cos_val * cx + sin_val * cy\n",
    "    ty = cy - sin_val * cx - cos_val * cy\n",
    "    # Construct transform vector for ImageProjectiveTransformV2: [a0, a1, a2, a3, a4, a5, 0, 0]\n",
    "    transform = [cos_val, -sin_val, tx, sin_val, cos_val, ty, 0, 0]\n",
    "    transforms = tf.stack([tf.stack(transform)])\n",
    "    output_shape = tf.shape(image)[1:3]\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV2(\n",
    "        images=image,\n",
    "        transforms=transforms,\n",
    "        output_shape=output_shape,\n",
    "        interpolation=\"BILINEAR\"\n",
    "    )\n",
    "    rotated = tf.squeeze(rotated, axis=0)\n",
    "    return rotated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_mask(mask, angle):\n",
    "    \"\"\"\n",
    "    Rotates a mask (tensor) by a given angle (in radians) using nearest neighbor interpolation.\n",
    "    \"\"\"\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "    cos_val = tf.math.cos(angle)\n",
    "    sin_val = tf.math.sin(angle)\n",
    "    h = tf.cast(tf.shape(mask)[1], tf.float32)\n",
    "    w = tf.cast(tf.shape(mask)[2], tf.float32)\n",
    "    cx = w / 2.0\n",
    "    cy = h / 2.0\n",
    "    tx = cx - cos_val * cx + sin_val * cy\n",
    "    ty = cy - sin_val * cx - cos_val * cy\n",
    "    transform = [cos_val, -sin_val, tx, sin_val, cos_val, ty, 0, 0]\n",
    "    transforms = tf.stack([tf.stack(transform)])\n",
    "    output_shape = tf.shape(mask)[1:3]\n",
    "    rotated = tf.raw_ops.ImageProjectiveTransformV2(\n",
    "        images=mask,\n",
    "        transforms=transforms,\n",
    "        output_shape=output_shape,\n",
    "        interpolation=\"NEAREST\"\n",
    "    )\n",
    "    rotated = tf.squeeze(rotated, axis=0)\n",
    "    return rotated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Paths & Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for images and corresponding segmentation masks\n",
    "image_dir = \"C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\Train\\\\R_HAM 10000 images\"\n",
    "mask_dir  = \"C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\Train\\\\R_segmentation\"\n",
    "\n",
    "# List all image files (assuming .jpg or .png) and sort them so that filenames match between images and masks\n",
    "all_image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
    "                           if f.lower().endswith(('.jpg', '.png'))])\n",
    "all_mask_files  = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)\n",
    "                           if f.lower().endswith(('.jpg', '.png'))])\n",
    "\n",
    "print(\"Total images:\", len(all_image_files))\n",
    "print(\"Total masks:\", len(all_mask_files))  # Should be 10015 each\n",
    "\n",
    "# Create a list of indices and shuffle them\n",
    "num_samples = len(all_image_files)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into 70:20:10 for training, validation, and testing\n",
    "train_split = int(0.7 * num_samples)\n",
    "val_split   = int(0.9 * num_samples)\n",
    "train_indices = indices[:train_split]\n",
    "val_indices   = indices[train_split:val_split]\n",
    "test_indices  = indices[val_split:]\n",
    "\n",
    "def get_file_list(indices, file_list):\n",
    "    return [file_list[i] for i in indices]\n",
    "\n",
    "train_images = get_file_list(train_indices, all_image_files)\n",
    "train_masks  = get_file_list(train_indices, all_mask_files)\n",
    "val_images   = get_file_list(val_indices, all_image_files)\n",
    "val_masks    = get_file_list(val_indices, all_mask_files)\n",
    "test_images  = get_file_list(test_indices, all_image_files)\n",
    "test_masks   = get_file_list(test_indices, all_mask_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_no_augmentation(image_path):\n",
    "    \"\"\"Loads image: read file, decode JPEG, resize to 256x256, convert to [0,1] float32.\"\"\"\n",
    "    image_data = tf.io.read_file(image_path)\n",
    "    image_data = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image_data = tf.image.resize(image_data, [256, 256])\n",
    "    image_data = tf.image.convert_image_dtype(image_data, tf.float32)\n",
    "    return image_data\n",
    "\n",
    "def load_mask_no_augmentation(mask_path):\n",
    "    \"\"\"Loads mask: read file, decode (assumed to be single-channel), resize (using NEAREST neighbor), and binarize.\"\"\"\n",
    "    mask_data = tf.io.read_file(mask_path)\n",
    "    # Use decode_image to support png/jpg; force single channel.\n",
    "    mask_data = tf.image.decode_image(mask_data, channels=1)\n",
    "    # Fix: set the shape so that resize can infer dimensions\n",
    "    mask_data.set_shape([None, None, 1])\n",
    "    mask_data = tf.image.resize(mask_data, [256, 256], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # Binarize mask: assuming pixel values in [0,255]; threshold at 127\n",
    "    mask_data = tf.cast(mask_data > 127, tf.float32)\n",
    "    return mask_data\n",
    "\n",
    "def load_image_and_mask(image_path, mask_path):\n",
    "    \"\"\"Loads and processes an image and its corresponding mask.\"\"\"\n",
    "    image = load_image_no_augmentation(image_path)\n",
    "    mask  = load_mask_no_augmentation(mask_path)\n",
    "    return image, mask\n",
    "\n",
    "# --- Modified: Data Augmentation Function ---\n",
    "def augment_image_and_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Applies random data augmentation to both image and mask.\n",
    "    Supports random horizontal & vertical flips, rotation, brightness, and contrast adjustments.\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "         image = tf.image.flip_left_right(image)\n",
    "         mask = tf.image.flip_left_right(mask)\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "         image = tf.image.flip_up_down(image)\n",
    "         mask = tf.image.flip_up_down(mask)\n",
    "    # Random rotation (angle between -0.3 and 0.3 radians)\n",
    "    angle = tf.random.uniform((), minval=-0.3, maxval=0.3)\n",
    "    image = rotate_image(image, angle)\n",
    "    mask = rotate_mask(mask, angle)\n",
    "    # Random brightness and contrast adjustments\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image, mask\n",
    "\n",
    "def create_dataset(image_paths, mask_paths, batch_size=16, shuffle=False, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
    "    dataset = dataset.map(lambda img, msk: load_image_and_mask(img, msk),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if augment:\n",
    "         dataset = dataset.map(lambda img, msk: augment_image_and_mask(img, msk),\n",
    "                                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 16\n",
    "# Use augmentation for the training dataset only.\n",
    "train_ds = create_dataset(train_images, train_masks, batch_size=batch_size, shuffle=True, augment=True)\n",
    "val_ds   = create_dataset(val_images, val_masks, batch_size=batch_size, shuffle=False, augment=False)\n",
    "test_ds  = create_dataset(test_images, test_masks, batch_size=batch_size, shuffle=False, augment=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Metrics: Dice Coefficient and Mean IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Computes Dice coefficient.\"\"\"\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "\n",
    "miou_metric = tf.keras.metrics.MeanIoU(num_classes=2)\n",
    "\n",
    "# --- Modified: Focal Tversky Loss Function ---\n",
    "def focal_tversky_loss(y_true, y_pred, alpha=0.8, beta=0.2, gamma=1.0, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Focal Tversky Loss, useful for segmenting small or fuzzy lesions.\n",
    "    \"\"\"\n",
    "    y_true_flat = tf.reshape(y_true, [-1])\n",
    "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
    "    TP = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    FP = tf.reduce_sum((1 - y_true_flat) * y_pred_flat)\n",
    "    FN = tf.reduce_sum(y_true_flat * (1 - y_pred_flat))\n",
    "    tversky_index = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)\n",
    "    return tf.pow((1 - tversky_index), gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. UNet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(256,256,3), dropout_rate=0.2):\n",
    "    inputs = layers.Input(input_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(32, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(inputs)\n",
    "    c1 = layers.Conv2D(32, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c1)\n",
    "    p1 = layers.MaxPooling2D((2,2))(c1)\n",
    "    p1 = layers.Dropout(dropout_rate)(p1)\n",
    "    \n",
    "    c2 = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(p1)\n",
    "    c2 = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c2)\n",
    "    p2 = layers.MaxPooling2D((2,2))(c2)\n",
    "    p2 = layers.Dropout(dropout_rate)(p2)\n",
    "    \n",
    "    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(p2)\n",
    "    c3 = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c3)\n",
    "    p3 = layers.MaxPooling2D((2,2))(c3)\n",
    "    p3 = layers.Dropout(dropout_rate)(p3)\n",
    "    \n",
    "    c4 = layers.Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(p3)\n",
    "    c4 = layers.Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c4)\n",
    "    p4 = layers.MaxPooling2D((2,2))(c4)\n",
    "    p4 = layers.Dropout(dropout_rate)(p4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(512, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(p4)\n",
    "    c5 = layers.Conv2D(512, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c5)\n",
    "    \n",
    "    # Decoder\n",
    "    u6 = layers.UpSampling2D((2,2))(c5)\n",
    "    u6 = layers.Conv2D(256, (2,2), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(u6)\n",
    "    merge6 = layers.concatenate([c4, u6])\n",
    "    c6 = layers.Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge6)\n",
    "    c6 = layers.Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c6)\n",
    "    \n",
    "    u7 = layers.UpSampling2D((2,2))(c6)\n",
    "    u7 = layers.Conv2D(128, (2,2), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(u7)\n",
    "    merge7 = layers.concatenate([c3, u7])\n",
    "    c7 = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge7)\n",
    "    c7 = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c7)\n",
    "    \n",
    "    u8 = layers.UpSampling2D((2,2))(c7)\n",
    "    u8 = layers.Conv2D(64, (2,2), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(u8)\n",
    "    merge8 = layers.concatenate([c2, u8])\n",
    "    c8 = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge8)\n",
    "    c8 = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c8)\n",
    "    \n",
    "    u9 = layers.UpSampling2D((2,2))(c8)\n",
    "    u9 = layers.Conv2D(32, (2,2), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(u9)\n",
    "    merge9 = layers.concatenate([c1, u9])\n",
    "    c9 = layers.Conv2D(32, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(merge9)\n",
    "    c9 = layers.Conv2D(32, (3,3), activation='relu', padding='same',\n",
    "                        kernel_regularizer=tf.keras.regularizers.l2(1e-4))(c9)\n",
    "    \n",
    "    outputs = layers.Conv2D(1, (1,1), activation='sigmoid')(c9)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Instantiate the UNet model\n",
    "unet = unet_model()\n",
    "# Print summary table\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Compile & Train the UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Focal Tversky Loss to better address class imbalance and improve segmentation.\n",
    "unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=focal_tversky_loss,\n",
    "              metrics=['accuracy', dice_coefficient, miou_metric])\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\"C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\2UNET.keras\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "earlystop_cb   = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "reduce_lr_cb   = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = unet.fit(train_ds,\n",
    "                   epochs=200,\n",
    "                   validation_data=val_ds,\n",
    "                   callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    epochs_range = range(len(history.history['accuracy']))\n",
    "    plt.figure(figsize=(14,10))\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(epochs_range, history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs_range, history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Accuracy')\n",
    "    \n",
    "    # Loss\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(epochs_range, history.history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs_range, history.history['val_loss'], label='Val Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Loss')\n",
    "    \n",
    "    # Dice Coefficient\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(epochs_range, history.history['dice_coefficient'], label='Train Dice')\n",
    "    plt.plot(epochs_range, history.history['val_dice_coefficient'], label='Val Dice')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Dice Coefficient')\n",
    "    \n",
    "    # Mean IoU (if available)\n",
    "    if 'mean_io_u' in history.history and 'val_mean_io_u' in history.history:\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.plot(epochs_range, history.history['mean_io_u'], label='Train mIoU')\n",
    "        plt.plot(epochs_range, history.history['val_mean_io_u'], label='Val mIoU')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title('Mean IoU')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Evaluate on Test Set & Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_dice, test_miou = unet.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test Dice: {test_dice:.4f}, Test mIoU: {test_miou:.4f}\")\n",
    "\n",
    "# For a pixel-wise confusion matrix, we compute over the test dataset.\n",
    "def compute_confusion_matrix(dataset, model):\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for images, masks in dataset:\n",
    "        preds = model.predict(images)\n",
    "        preds = (preds > 0.5).astype(np.uint8)\n",
    "        y_trues.append(masks.numpy().flatten())\n",
    "        y_preds.append(preds.flatten())\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    cm = confusion_matrix(y_trues, y_preds)\n",
    "    return cm\n",
    "\n",
    "cm = compute_confusion_matrix(test_ds, unet)\n",
    "print(\"Pixel-wise Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Pixel-wise Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Save the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.save(\"C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\2UNET.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable oneDNN optimizations (optional)\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define the 7 classes and mapping from class name to integer label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = ['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec']\n",
    "num_classes = len(class_names)\n",
    "label_mapping = {name: idx for idx, name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths for images & metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_image_dir = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\2Train\"\n",
    "train_metadata_path = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Train_metadata.csv\"\n",
    "val_image_dir = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Validation\"\n",
    "val_metadata_path = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\2Validation_metadata.csv\"\n",
    "test_image_dir = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Test\"\n",
    "test_metadata_path = \"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\Test_metadata.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load & preprocess an image given its path and dx label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_label(image_path, dx):\n",
    "    try:\n",
    "        image_data = tf.io.read_file(image_path)\n",
    "        image_data = tf.image.decode_jpeg(image_data, channels=3)  # Load image\n",
    "        image_data = tf.image.resize(image_data, [256, 256])       # Ensure it is 256x256\n",
    "        image_data = tf.image.convert_image_dtype(image_data, tf.float32)  # Normalize to [0, 1]\n",
    "        dx = tf.cast(dx, tf.int32)\n",
    "        return image_data, dx\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return tf.zeros([256, 256, 3], dtype=tf.float32), tf.constant(-1, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tf.data.Dataset from metadata CSV and image directory using slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataset(image_dir, metadata_path, batch_size=32, shuffle=True, sample_count=None):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    # If sample_count is provided, slice the dataframe\n",
    "    if sample_count is not None:\n",
    "        df = df.iloc[:sample_count]\n",
    "    # Map string dx values to integer labels if necessary\n",
    "    if df['dx'].dtype == object:\n",
    "        df['dx'] = df['dx'].map(label_mapping)\n",
    "    image_paths = df['image_id'].apply(lambda x: os.path.join(image_dir, x + \".jpg\")).tolist()\n",
    "    labels = df['dx'].tolist()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    ds = ds.map(lambda path, dx: load_image_label(path, dx),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=12000, seed=42)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets for training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m(train_image_dir, train_metadata_path, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12000\u001b[39m)\n\u001b[0;32m      3\u001b[0m val_ds   \u001b[38;5;241m=\u001b[39m create_dataset(val_image_dir, val_metadata_path, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sample_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6000\u001b[39m)\n\u001b[0;32m      4\u001b[0m test_ds  \u001b[38;5;241m=\u001b[39m create_dataset(test_image_dir, test_metadata_path, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sample_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "train_ds = create_dataset(train_image_dir, train_metadata_path, batch_size=batch_size, shuffle=True, sample_count=12000)\n",
    "val_ds   = create_dataset(val_image_dir, val_metadata_path, batch_size=batch_size, shuffle=False, sample_count=6000)\n",
    "test_ds  = create_dataset(test_image_dir, test_metadata_path, batch_size=batch_size, shuffle=False, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation for the training dataset (modified for lower intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    # Reduced brightness and contrast augmentation\n",
    "    image = tf.image.random_brightness(image, max_delta=0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    # Limit rotation to 0° or 90° (instead of 0°, 90°, 180°, 270°)\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=2, dtype=tf.int32)\n",
    "    image = tf.image.rot90(image, k)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check a batch's shapes\n",
    "for img_batch, dx_batch in train_ds.take(1):\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")   # Expected: (32, 256, 256, 3)\n",
    "    print(f\"DX batch shape: {dx_batch.shape}\")         # Expected: (32,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an enhanced CNN model for multi-class classification using softmax.\n",
    "Adjusted architecture to reduce over-regularization and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_cnn(input_shape, num_classes):\n",
    "    # 0.0005 L2 regularization factor\n",
    "    l2_reg = tf.keras.regularizers.l2(0.0005)\n",
    "    model = models.Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        # Block 1\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Block 3\n",
    "        layers.Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Block 4 (deeper layer)\n",
    "        layers.Conv2D(256, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3,3), activation='relu', padding='same', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=l2_reg),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(num_classes, activation='softmax', kernel_regularizer=l2_reg)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (256, 256, 3)\n",
    "cnn_model = build_cnn(input_shape, num_classes)\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model without precision and recall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping callback based on validation loss (increased patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Learning Rate Scheduler callback to reduce LR when validation loss plateaus\n",
    "# ---------------------------------------\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-06)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcnn_model\u001b[49m\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      3\u001b[0m     train_ds, \n\u001b[0;32m      4\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_ds, \n\u001b[0;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS, \n\u001b[0;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, reduce_lr], \n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 150\n",
    "history = cnn_model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[early_stopping, reduce_lr], \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model as a Keras file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model.save(\"C:\\\\Users\\\\arjun\\\\OneDrive\\\\Desktop\\\\CNN_Model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = cnn_model.evaluate(test_ds)\n",
    "for name, value in zip(cnn_model.metrics_names, results):\n",
    "    print(f\"Test {name}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training vs. validation curves for loss and metrics (excluding precision and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m metrics_to_plot \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics_to_plot):\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_to_plot = [\"loss\", \"accuracy\", \"sparse_categorical_accuracy\"]\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(history.history[metric], label=f\"Train {metric}\")\n",
    "    plt.plot(history.history[f\"val_{metric}\"], label=f\"Val {metric}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Train vs Validation {metric}\")\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions on the test set and compute a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)\n",
    "y_pred_probs = cnn_model.predict(test_ds)\n",
    "y_pred = np.argmax(y_pred_probs, axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate F1 Score manually using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Test F1 Score (Macro Average): {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

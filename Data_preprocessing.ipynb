{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply random augmentations to an image including rotation, mirroring, brightness/contrast,\n",
    "stretching, and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    augmented_image = next(datagen.flow(image, batch_size=1))[0].astype(np.uint8)\n",
    "    return augmented_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Processes images based on metadata so that each class ends up with exactly 5000 images.\n",
    "    For classes with > 5000 images, it randomly samples 5000 images and copies them.\n",
    "    For classes with < 5000 images, it copies all originals and augments additional images until 5000.\n",
    "    Augmented image file names are generated using the original image ID plus a unique augmentation\n",
    "    counter, and these names are used consistently in the saved file and the metadata CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to Processed_HAM10000_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(metadata_path, image_folder, output_folder, output_metadata_path):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    df = pd.read_csv(metadata_path)\n",
    "    new_metadata = []\n",
    "    classes = df['dx'].unique()\n",
    "\n",
    "    # Define the possible extensions for image files\n",
    "    possible_extensions = ['.jpg', '.jpeg', '.png']\n",
    "\n",
    "    for dx_class in classes:\n",
    "        # Get all image IDs for the current class\n",
    "        class_images = df[df['dx'] == dx_class]['image_id'].tolist()\n",
    "        final_images = []  # List to store image IDs that are saved for this class\n",
    "\n",
    "        if len(class_images) >= 5000:\n",
    "            # For classes with 5000 or more images, randomly sample 5000 images\n",
    "            selected_images = random.sample(class_images, 5000)\n",
    "            for image_id in selected_images:\n",
    "                # Find the original image file using the possible extensions\n",
    "                image_path = None\n",
    "                for ext in possible_extensions:\n",
    "                    temp_path = os.path.join(image_folder, image_id + ext)\n",
    "                    if os.path.exists(temp_path):\n",
    "                        image_path = temp_path\n",
    "                        break\n",
    "                if image_path is None:\n",
    "                    print(f\"Image for {image_id} not found. Skipping.\")\n",
    "                    continue\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    print(f\"Could not read image {image_path}. Skipping.\")\n",
    "                    continue\n",
    "                new_image_id = image_id  # No augmentation needed; keep original name\n",
    "                new_image_path = os.path.join(output_folder, new_image_id + '.jpg')\n",
    "                cv2.imwrite(new_image_path, image)\n",
    "                final_images.append(new_image_id)\n",
    "        else:\n",
    "            # For classes with fewer than 5000 images, copy all originals first\n",
    "            for image_id in class_images:\n",
    "                image_path = None\n",
    "                for ext in possible_extensions:\n",
    "                    temp_path = os.path.join(image_folder, image_id + ext)\n",
    "                    if os.path.exists(temp_path):\n",
    "                        image_path = temp_path\n",
    "                        break\n",
    "                if image_path is None:\n",
    "                    print(f\"Image for {image_id} not found. Skipping.\")\n",
    "                    continue\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    print(f\"Could not read image {image_path}. Skipping.\")\n",
    "                    continue\n",
    "                new_image_id = image_id  # Original image name\n",
    "                new_image_path = os.path.join(output_folder, new_image_id + '.jpg')\n",
    "                cv2.imwrite(new_image_path, image)\n",
    "                final_images.append(new_image_id)\n",
    "\n",
    "            # Initialize an augmentation counter for the class\n",
    "            aug_counter = 1\n",
    "            # Augment images until we have exactly 5000 for the class\n",
    "            while len(final_images) < 5000:\n",
    "                # Randomly choose one of the original images to augment\n",
    "                base_image_id = random.choice(class_images)\n",
    "                image_path = None\n",
    "                for ext in possible_extensions:\n",
    "                    temp_path = os.path.join(image_folder, base_image_id + ext)\n",
    "                    if os.path.exists(temp_path):\n",
    "                        image_path = temp_path\n",
    "                        break\n",
    "                if image_path is None:\n",
    "                    continue  # Skip if the file is not found\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                # Convert image to RGB for augmentation and then back later\n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                augmented_image = augment_image(image_rgb)\n",
    "                new_image_id = f\"{base_image_id}_aug{aug_counter}\"\n",
    "                aug_counter += 1\n",
    "                new_image_path = os.path.join(output_folder, new_image_id + '.jpg')\n",
    "                cv2.imwrite(new_image_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "                final_images.append(new_image_id)\n",
    "\n",
    "        # Append metadata for all images saved for this class.\n",
    "        # Their dx value is the same as the current dx_class.\n",
    "        for image_id in final_images:\n",
    "            new_metadata.append([image_id, dx_class])\n",
    "\n",
    "    # Save the metadata CSV\n",
    "    new_df = pd.DataFrame(new_metadata, columns=['image_id', 'dx'])\n",
    "    new_df.to_csv(output_metadata_path, index=False)\n",
    "    print(f\"Processed dataset saved to {output_metadata_path}\")\n",
    "\n",
    "process_dataset(\n",
    "    r\"HAM10000_metadata.csv\",\n",
    "    r\"c:\\Users\\Amrit Shah\\Downloads\\R_HAM 10000 images\",\n",
    "    r\"Processed_images\",\n",
    "    r\"Processed_HAM10000_metadata.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to the original dataset and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = r\"Processed_images\"\n",
    "metadata_path = r\"Processed_HAM10000_metadata.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output folders for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\train\"\n",
    "val_folder   = r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\val\"\n",
    "test_folder  = r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\test\"\n",
    "\n",
    "# Create the output folders if they don't exist\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(val_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata CSV\n",
    "df = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, split the data into training (70%) and temporary (30%) sets with stratification by 'dx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_temp = train_test_split(df, test_size=0.30, random_state=42, stratify=df['dx'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the temporary set into validation and test sets.\n",
    "We want overall 20% for validation and 10% for testing. Since df_temp is 30% of the data.\n",
    "we split df_temp into ~66.67% validation and ~33.33% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val, df_test = train_test_split(df_temp, test_size=0.3333, random_state=42, stratify=df_temp['dx'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of samples in each split for confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 24500\n",
      "Number of validation images: 7000\n",
      "Number of testing images: 3500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training images:\", len(df_train))\n",
    "print(\"Number of validation images:\", len(df_val))\n",
    "print(\"Number of testing images:\", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to copy image files based on metadata entries into a specified destination folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(metadata_df, dest_folder):\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        image_id = row['image_id']\n",
    "        # Assuming the images are saved as .jpg files\n",
    "        src_path = os.path.join(dataset_folder, image_id + \".jpg\")\n",
    "        dest_path = os.path.join(dest_folder, image_id + \".jpg\")\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_path)\n",
    "        else:\n",
    "            print(f\"Warning: {src_path} not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy images for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_images(df_train, train_folder)\n",
    "copy_images(df_val, val_folder)\n",
    "copy_images(df_test, test_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the corresponding metadata for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and metadata successfully split into training, validation, and testing sets.\n"
     ]
    }
   ],
   "source": [
    "df_train.to_csv(r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\train_metadata.csv\", index=False)\n",
    "df_val.to_csv(r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\val_metadata.csv\", index=False)\n",
    "df_test.to_csv(r\"c:\\Users\\Amrit Shah\\Desktop\\Minor Project\\Skin-Lesion-Classification-and-Segmentation\\test_metadata.csv\", index=False)\n",
    "\n",
    "print(\"Dataset and metadata successfully split into training, validation, and testing sets.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

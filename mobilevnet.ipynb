{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3287d623",
   "metadata": {},
   "source": [
    "# Skin lesion Classification and segementation using the pretrianed model RESNET 50 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee000c5",
   "metadata": {},
   "source": [
    "Importing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1a2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f42f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\train\"\n",
    "train_metadata_path = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\Skin-Lesion-Classification-and-Segmentation\\train_metadata.csv\"\n",
    "val_image_dir = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\val\"\n",
    "val_metadata_path = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\Skin-Lesion-Classification-and-Segmentation\\val_metadata.csv\"\n",
    "test_image_dir = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\test\"\n",
    "test_metadata_path = r\"C:\\Users\\Administrator\\Desktop\\Abhishek\\Skin-Lesion-Classification-and-Segmentation\\test_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303bbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_label(image_path, dx):\n",
    "    try:\n",
    "        image_data = tf.io.read_file(image_path)\n",
    "        image_data = tf.image.decode_jpeg(image_data, channels=3)  # Load image\n",
    "        image_data = tf.image.resize(image_data, [256, 256])       # Ensure it is 256x256\n",
    "        image_data = tf.image.convert_image_dtype(image_data, tf.float32)  # Normalize to [0, 1]\n",
    "        dx = tf.cast(dx, tf.int32)\n",
    "        return image_data, dx\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {image_path}: {e}\")\n",
    "        return tf.zeros([256, 256, 3], dtype=tf.float32), tf.constant(-1, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d04735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(image_dir, metadata_path, batch_size=32, shuffle=True, sample_count=None):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    if sample_count is not None:\n",
    "        df = df.iloc[:sample_count]\n",
    "\n",
    "    # Automatically create label mapping\n",
    "    if df['dx'].dtype == object:\n",
    "        unique_labels = sorted(df['dx'].unique())\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        df['dx'] = df['dx'].map(label_mapping)\n",
    "\n",
    "    image_paths = df['image_id'].apply(lambda x: os.path.join(image_dir, x + \".jpg\")).tolist()\n",
    "    labels = df['dx'].tolist()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    ds = ds.map(lambda path, dx: load_image_label(path, dx),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=sample_count or len(df), seed=42)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99c841c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "train_ds = create_dataset(train_image_dir, train_metadata_path, batch_size=batch_size, shuffle=True, sample_count=12000)\n",
    "val_ds   = create_dataset(val_image_dir, val_metadata_path, batch_size=batch_size, shuffle=False, sample_count=6000)\n",
    "test_ds  = create_dataset(test_image_dir, test_metadata_path, batch_size=batch_size, shuffle=False, sample_count=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968e4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    # Reduced brightness and contrast augmentation\n",
    "    image = tf.image.random_brightness(image, max_delta=0.05)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    # Limit rotation to 0¬∞ or 90¬∞ (instead of 0¬∞, 90¬∞, 180¬∞, 270¬∞)\n",
    "    k = tf.random.uniform(shape=[], minval=0, maxval=2, dtype=tf.int32)\n",
    "    image = tf.image.rot90(image, k)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55439866",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: C:\\Users\\Administrator\\Desktop\\Abhishek\\train\\ISIC_0028880_aug3014.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\Skin-Lesion-Classification-and-Segmentation\\mobilevnet.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Debug: Check a batch's shapes\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m img_batch, dx_batch \u001b[39min\u001b[39;00m train_ds\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mImage batch shape: \u001b[39m\u001b[39m{\u001b[39;00mimg_batch\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)   \u001b[39m# Expected: (32, 256, 256, 3)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDX batch shape: \u001b[39m\u001b[39m{\u001b[39;00mdx_batch\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)         \u001b[39m# Expected: (32,)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    813\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[0;32m    815\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    816\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[1;32m--> 777\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[0;32m    778\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[0;32m    779\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[0;32m    780\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[0;32m    782\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3055\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3053\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3054\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 3055\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   3056\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   3057\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6654\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   6655\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m-> 6656\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: C:\\Users\\Administrator\\Desktop\\Abhishek\\train\\ISIC_0028880_aug3014.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# Debug: Check a batch's shapes\n",
    "for img_batch, dx_batch in train_ds.take(1):\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")   # Expected: (32, 256, 256, 3)\n",
    "    print(f\"DX batch shape: {dx_batch.shape}\")         # Expected: (32,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3dc9c",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e43f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 8, 8, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               327936    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2587719 (9.87 MB)\n",
      "Trainable params: 329735 (1.26 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_classes = 7  # Update based on your dataset\n",
    "\n",
    "# Load pre-trained MobileNetV2 (without the top classification layer)\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(256, 256, 3)\n",
    ")\n",
    "\n",
    "# Freeze base layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Define the model function\n",
    "def mobilenet(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')  # ‚úÖ Using num_classes directly\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and summarize the model\n",
    "input_shape = (256, 256, 3)\n",
    "mobilenet_model = mobilenet(input_shape, num_classes)\n",
    "mobilenet_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Resuming from checkpoint: ./checkpoints\\mob-02.keras\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# === 1. Settings ===\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_pattern = os.path.join(checkpoint_dir, \"mob-{epoch:02d}.keras\")\n",
    "\n",
    "# === 2. Get latest checkpoint (if any) ===\n",
    "def get_latest_checkpoint():\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"mob\") and f.endswith(\".keras\")]\n",
    "    if not files:\n",
    "        return None, 0\n",
    "    def extract_epoch(fname):\n",
    "        match = re.search(r'mob-(\\d+)\\.keras', fname)\n",
    "        return int(match.group(1)) if match else -1\n",
    "    latest_file = max(files, key=extract_epoch)\n",
    "    latest_epoch = extract_epoch(latest_file)\n",
    "    return os.path.join(checkpoint_dir, latest_file), latest_epoch\n",
    "\n",
    "latest_checkpoint_path, latest_epoch = get_latest_checkpoint()\n",
    "\n",
    "# === 3. Load or build model ===\n",
    "if latest_checkpoint_path:\n",
    "    print(f\"üîÅ Resuming from checkpoint: {latest_checkpoint_path}\")\n",
    "    mobilenet_model = load_model(latest_checkpoint_path)\n",
    "else:\n",
    "    print(\"üÜï Starting fresh training...\")\n",
    "    mobilenet_model = mobilenet(input_shape, num_classes)  # You must define this function\n",
    "\n",
    "# === 4. Custom callback to save every 15 epochs ===\n",
    "class SaveEveryNEpochs(Callback):\n",
    "    def __init__(self, save_freq, checkpoint_template):\n",
    "        super().__init__()\n",
    "        self.save_freq = save_freq\n",
    "        self.checkpoint_template = checkpoint_template\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.save_freq == 0:\n",
    "            filepath = self.checkpoint_template.format(epoch=epoch + 1)\n",
    "            self.model.save(filepath)\n",
    "            print(f\"‚úÖ Saved full model at {filepath}\")\n",
    "\n",
    "checkpoint_callback = SaveEveryNEpochs(save_freq=15, checkpoint_template=checkpoint_pattern)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mobilenet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\", SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b610f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Learning Rate Scheduler callback to reduce LR when validation loss plateaus\n",
    "# ---------------------------------------\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-06)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/150\n",
      " 11/375 [..............................] - ETA: 2:35 - loss: 1.5582 - accuracy: 0.3608 - sparse_categorical_accuracy: 0.3608"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\Skin-Lesion-Classification-and-Segmentation\\mobilevnet.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m150\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m mobilenet_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_ds, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_ds, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mEPOCHS, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49mlatest_epoch,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping, reduce_lr,checkpoint_callback], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/Desktop/Abhishek/Skin-Lesion-Classification-and-Segmentation/mobilevnet.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\Desktop\\Abhishek\\tf-venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 150\n",
    "history = mobilenet_model.fit(\n",
    "    train_ds, \n",
    "    validation_data=val_ds, \n",
    "    epochs=EPOCHS, \n",
    "    initial_epoch=latest_epoch,\n",
    "    callbacks=[early_stopping, reduce_lr,checkpoint_callback], \n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
